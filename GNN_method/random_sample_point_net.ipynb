{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "samplenumber = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6493a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class data_transform:\n",
    "    \n",
    "    #d: the number of decimals reserved\n",
    "    #n: the number of the nearest points selected around the feature point\n",
    "    #path_data_file: the direction of the folder where contains the txt.\n",
    "    def __init__(self, path_data_file):\n",
    "        self.path_data_file = path_data_file\n",
    "\n",
    "\n",
    "    \n",
    "    # import all the files in the folder as dataframe, and name them as df_n\n",
    "    def get_file(self):\n",
    "        file_path = self.path_data_file\n",
    "        augmentated_list = []\n",
    "        file_list = []\n",
    "        \n",
    "        for i in os.listdir(file_path):\n",
    "            file_list.append(os.path.join(file_path, i))\n",
    "\n",
    "        \n",
    "        names = locals()\n",
    "        list_df = []\n",
    "\n",
    "        \n",
    "        for j in range(len(file_list)):\n",
    "            names['df_%s'%j] = pd.read_table(file_list[j], sep='\\t', skiprows=0)\n",
    "            names['df_%s'%j]['Z'] = [0]*len(names['df_%s'%j])\n",
    "            names['df_%s'%j]['PT'] = names['df_%s'%j]['PT'].astype(str)\n",
    "            names['df_%s'%j] = names['df_%s'%j].sample( n = samplenumber, replace= False, axis = 0)\n",
    "            list_df.append(names['df_%s'%j])\n",
    "            \n",
    "        return list_df\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    def generate_data(self, list_df_trans):\n",
    "        point_clouds, point_cloud_labels = [],[]\n",
    "        for df in tqdm(list_df_trans):\n",
    "            df.H = df.H/4001\n",
    "            df.V = df.V/3001\n",
    "\n",
    "            point_cloud = df[['H', 'V', 'Z']].values\n",
    "            point_clouds.append(point_cloud)\n",
    "            \n",
    "            label = pd.get_dummies(df.PT, drop_first=False)\n",
    "            labels = label.columns.values\n",
    "            labels_all = [str(x) for x in range(6)]\n",
    "            complementaryset = list(set(labels_all) - set(labels))\n",
    "\n",
    "            for PT in complementaryset:\n",
    "                column_new = [0] * len(label)\n",
    "                label[str(PT)] = column_new\n",
    "                label[str(PT)] = label[str(PT)].astype(np.uint8)\n",
    "                \n",
    "            label = label.sort_index(axis = 1)\n",
    "            label = label.values\n",
    "            point_cloud_labels.append(label)\n",
    "            \n",
    "        return point_clouds, point_cloud_labels\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = '/Users/agustinzhang/Downloads/master_AI/TFM/Dato/Datos_pointnet/'\n",
    "file_path_train = file_path+'train'\n",
    "file_path_test = file_path+'test'\n",
    "\n",
    "trans = data_transform(file_path_train)\n",
    "tests = data_transform(file_path_test)\n",
    "list_df_trans = trans.get_file()\n",
    "list_df_trans_test = tests.get_file()\n",
    "\n",
    "my_point_clouds, my_point_cloud_labels = trans.generate_data(list_df_trans)\n",
    "my_point_clouds_test, my_point_cloud_labels_test = trans.generate_data(list_df_trans_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_data(point_cloud, labels):\n",
    "    COLORS = ['red', 'green', 'blue', 'purple', 'black', 'orange']\n",
    "    df = pd.DataFrame(\n",
    "        data={\n",
    "            \"x\": point_cloud[:, 0],\n",
    "            \"y\": point_cloud[:, 1],\n",
    "            \"z\": point_cloud[:, 2],\n",
    "            \"label\": labels,\n",
    "        }\n",
    "    )\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    for index, label in enumerate(['0', '1', '2', '3', '4', '5']):\n",
    "        c_df = df[df[\"label\"] == label]\n",
    "        try:\n",
    "            ax.scatter(\n",
    "                c_df[\"x\"], c_df[\"y\"], c_df[\"z\"], label=label, alpha=0.5, c=COLORS[index]\n",
    "            )\n",
    "        except IndexError:\n",
    "            pass\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_data(my_point_clouds[587], list_df_trans[587].PT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 60\n",
    "INITIAL_LR = 1e-3\n",
    "\n",
    "\n",
    "def load_data(point_cloud_batch, label_cloud_batch):\n",
    "    point_cloud_batch.set_shape([samplenumber, 3])\n",
    "    label_cloud_batch.set_shape([samplenumber, 6])\n",
    "    return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "def augment(point_cloud_batch, label_cloud_batch):\n",
    "    noise = tf.random.uniform(\n",
    "        tf.shape(label_cloud_batch), -0.005, 0.005, dtype=tf.float64\n",
    "    )\n",
    "    point_cloud_batch += noise[:, :, :3]\n",
    "    return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "\n",
    "def generate_dataset(point_clouds, label_clouds, is_training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((point_clouds, label_clouds))\n",
    "    dataset = dataset.shuffle(BATCH_SIZE * 100) if is_training else dataset\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
    "    dataset = (\n",
    "        dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if is_training\n",
    "        else dataset\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "train_point_clouds = my_point_clouds\n",
    "train_label_cloud = my_point_cloud_labels\n",
    "total_training_examples = len(train_point_clouds)\n",
    "\n",
    "val_point_clouds = my_point_clouds_test\n",
    "val_label_cloud = my_point_cloud_labels_test\n",
    "\n",
    "print(\"Num train point clouds:\", len(train_point_clouds))\n",
    "print(\"Num train point cloud labels:\", len(train_label_cloud))\n",
    "print(\"Num val point clouds:\", len(val_point_clouds))\n",
    "print(\"Num val point cloud labels:\", len(val_label_cloud))\n",
    "    \n",
    "train_dataset = generate_dataset(train_point_clouds, train_label_cloud)\n",
    "val_dataset = generate_dataset(val_point_clouds, val_label_cloud, is_training=False)\n",
    "    \n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Validation Dataset:\", val_dataset)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_block(x: tf.Tensor, filters: int, name: str) -> tf.Tensor:\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\", name=f\"{name}_conv\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0, name=f\"{name}_batch_norm\")(x)\n",
    "    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n",
    "\n",
    "\n",
    "def mlp_block(x: tf.Tensor, filters: int, name: str) -> tf.Tensor:\n",
    "    x = layers.Dense(filters, name=f\"{name}_dense\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0, name=f\"{name}_batch_norm\")(x)\n",
    "    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n",
    "    \n",
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    \"\"\"Reference: https://keras.io/examples/vision/pointnet/#build-a-model\"\"\"\n",
    "\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.identity = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.identity))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerEncoder, self).get_config()\n",
    "        config.update({\"num_features\": self.num_features, \"l2reg_strength\": self.l2reg})\n",
    "        return config    \n",
    "    \n",
    "def transformation_net(inputs: tf.Tensor, num_features: int, name: str) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Reference: https://keras.io/examples/vision/pointnet/#build-a-model.\n",
    "\n",
    "    The `filters` values come from the original paper:\n",
    "    https://arxiv.org/abs/1612.00593.\n",
    "    \"\"\"\n",
    "    x = conv_block(inputs, filters=64, name=f\"{name}_1\")\n",
    "    x = conv_block(x, filters=128, name=f\"{name}_2\")\n",
    "    x = conv_block(x, filters=1024, name=f\"{name}_3\")\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = mlp_block(x, filters=512, name=f\"{name}_1_1\")\n",
    "    x = mlp_block(x, filters=256, name=f\"{name}_2_1\")\n",
    "    return layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=keras.initializers.Constant(np.eye(num_features).flatten()),\n",
    "        activity_regularizer=OrthogonalRegularizer(num_features),\n",
    "        name=f\"{name}_final\",\n",
    "    )(x)\n",
    "\n",
    "\n",
    "def transformation_block(inputs: tf.Tensor, num_features: int, name: str) -> tf.Tensor:\n",
    "    transformed_features = transformation_net(inputs, num_features, name=name)\n",
    "    transformed_features = layers.Reshape((num_features, num_features))(\n",
    "        transformed_features\n",
    "    )\n",
    "    return layers.Dot(axes=(2, 1), name=f\"{name}_mm\")([inputs, transformed_features])\n",
    "    \n",
    "    \n",
    "def get_shape_segmentation_model(num_points: int, num_classes: int) -> keras.Model:\n",
    "    input_points = keras.Input(shape=(None, 3))\n",
    "\n",
    "    # PointNet Classification Network.\n",
    "    transformed_inputs = transformation_block(\n",
    "        input_points, num_features=3, name=\"input_transformation_block\"\n",
    "    )\n",
    "    features_64 = conv_block(transformed_inputs, filters=64, name=\"features_64\")\n",
    "    features_128_1 = conv_block(features_64, filters=128, name=\"features_128_1\")\n",
    "    features_128_2 = conv_block(features_128_1, filters=128, name=\"features_128_2\")\n",
    "    transformed_features = transformation_block(\n",
    "        features_128_2, num_features=128, name=\"transformed_features\"\n",
    "    )\n",
    "    features_512 = conv_block(transformed_features, filters=512, name=\"features_512\")\n",
    "    features_2048 = conv_block(features_512, filters=2048, name=\"pre_maxpool_block\")\n",
    "    global_features = layers.MaxPool1D(pool_size=num_points, name=\"global_features\")(\n",
    "        features_2048\n",
    "    )\n",
    "    global_features = tf.tile(global_features, [1, num_points, 1])\n",
    "\n",
    "    # Segmentation head.\n",
    "    segmentation_input = layers.Concatenate(name=\"segmentation_input\")(\n",
    "        [\n",
    "            features_64,\n",
    "            features_128_1,\n",
    "            features_128_2,\n",
    "            transformed_features,\n",
    "            features_512,\n",
    "            global_features,\n",
    "        ]\n",
    "    )\n",
    "    segmentation_features = conv_block(\n",
    "        segmentation_input, filters=128, name=\"segmentation_features\"\n",
    "    )\n",
    "    outputs = layers.Conv1D(\n",
    "        num_classes, kernel_size=1, activation=\"softmax\", name=\"segmentation_head\"\n",
    "    )(segmentation_features)\n",
    "    return keras.Model(input_points, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c934161",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "x, y = next(iter(train_dataset))\n",
    "\n",
    "num_points = x.shape[1]\n",
    "num_classes = y.shape[-1]\n",
    "\n",
    "segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n",
    "segmentation_model.summary()\n",
    "\n",
    "training_step_size = total_training_examples // BATCH_SIZE\n",
    "total_training_steps = training_step_size * EPOCHS\n",
    "print(f\"Total training steps: {total_training_steps}.\")\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[training_step_size * 15, training_step_size * 15],\n",
    "    values=[INITIAL_LR, INITIAL_LR * 0.5, INITIAL_LR * 0.25],\n",
    ")\n",
    "\n",
    "steps = tf.range(total_training_steps, dtype=tf.int32)\n",
    "lrs = [lr_schedule(step) for step in steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288160ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(epochs):\n",
    "\n",
    "    segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n",
    "    segmentation_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = segmentation_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    segmentation_model.load_weights(checkpoint_filepath)\n",
    "    return segmentation_model, history\n",
    "\n",
    "\n",
    "segmentation_model, history = run_experiment(epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59782c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97010fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_batch = next(iter(val_dataset))\n",
    "val_predictions = segmentation_model.predict(validation_batch[0])\n",
    "print(f\"Validation prediction shape: {val_predictions.shape}\")\n",
    "\n",
    "\n",
    "def visualize_single_point_cloud(point_clouds, label_clouds, idx):\n",
    "    label_map = ['0', '1', '2', '3', '4', '5'] + [\"none\"]\n",
    "    point_cloud = point_clouds[idx]\n",
    "    label_cloud = label_clouds[idx]\n",
    "    visualize_data(point_cloud, [label_map[np.argmax(label)] for label in label_cloud])\n",
    "\n",
    "\n",
    "idx = np.random.choice(len(validation_batch[0]))\n",
    "print(f\"Index selected: {idx}\")\n",
    "\n",
    "# Plotting with ground-truth.\n",
    "visualize_single_point_cloud(validation_batch[0], validation_batch[1], idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876be8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with predicted labels.\n",
    "visualize_single_point_cloud(validation_batch[0], val_predictions, idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
